{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Embedded PDFS from a website that uses pdfemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import base64\n",
    "from fpdf import FPDF\n",
    "import random\n",
    "import logging\n",
    "import pandas as pd\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a Logging File to track the success of our scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"logfileerror.log\", level=logging.INFO) # Create log file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a function which adds whitepaper name and corresponding URL to our hashmap dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note the function references globally defined variables driver and hashmap which we expect to already be initialised.\n",
    "def addtohashmap():\n",
    "    #Get Links\n",
    "    Links = driver.find_elements(By.LINK_TEXT, 'Whitepaper') #Get all links to whitepaper in order that they appear in the table\n",
    "    Linkstext = [elem.get_attribute(\"href\") for elem in Links] #Get hyperlink\n",
    "\n",
    "    #Get labels\n",
    "    tags = driver.find_elements(By.TAG_NAME, 'td')  #Search for td tagged elements in the order that they appear\n",
    "    Labelselem = []\n",
    "    for elem in tags:\n",
    "        if elem.text !=\"Whitepaper\": #Only want the text in the table which doesn't refer to the whitepaper links \n",
    "            Labelselem.append(elem)\n",
    "    Labelstext = [elem.text for elem in Labelselem] #Convert Selenium Elements to text\n",
    "\n",
    "\n",
    "    #add label and links to our hashmap\n",
    "    if len(Linkstext) != len(Labelstext):\n",
    "        raise Exception(\"Number of titles don't match number of rows\")#If the number of whitepapers is\n",
    "        # more or less than the number of labels. There may be a misalignment.\n",
    "    for i in range(len(Links)):\n",
    "        if Labelstext[i] not in hashmap:\n",
    "            hashmap[Labelstext[i]] =  Linkstext[i]\n",
    "        else:\n",
    "            hashmap[Labelstext[i] + \"DUPLICATE\"] = Linkstext[i] #If one name has 2 or more different links, we do not want to \n",
    "                #overwrite the data and lose one of the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open Chrome window using chromium\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.maximize_window()\n",
    "\n",
    "driver.get(\"https://www.allcryptowhitepapers.com/whitepaper-overview/\") #Gets link to the table\n",
    "xpath = \"//a[contains(@class,'paginate_button next')]\" #Locates next button\n",
    "\n",
    "while(True):\n",
    "    addtohashmap() #Scan page for table data and add this to hashmap\n",
    "    \n",
    "    #Finds next button and clicks it\n",
    "    #Note that the next button is of aria type meaning that we need to do an additional step to click on button\n",
    "    cls = driver.find_element_by_xpath(xpath).get_attribute(\"class\")\n",
    "    if 'disabled' not in cls:\n",
    "        driver.find_element_by_xpath(xpath).click()\n",
    "    else:\n",
    "        break\n",
    "\n",
    "#Save all to dataframe\n",
    "df = pd.DataFrame()\n",
    "df['Whitepaper Name'] = list(hashmap.keys())\n",
    "df['Links'] = list(hashmap.values())\n",
    "\n",
    "df.to_csv('Whitepaperswithlinks.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct hashmap from the file that we may have originally stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\".\\Whitepaperswithlinks.csv\")\n",
    "names = list(df['Whitepaper Name'])\n",
    "link = list(df['Links'])\n",
    "hashmap = {}\n",
    "for i in range(0,len(names)):\n",
    "    hashmap[names[i]] = link[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run our scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.maximize_window()\n",
    "\n",
    "#For each link in our hashmap we would like to scrape our whitepaper\n",
    "for t in range(0,len(hashmap)):\n",
    "    #Navigate to our weblink\n",
    "    whitepaper = list(hashmap.keys())[t]\n",
    "    driver.get(hashmap[whitepaper])\n",
    "\n",
    "    #Wait for webpage to load\n",
    "    time.sleep(10)\n",
    "    label = driver.find_element(By.XPATH, \"//h1[@class = 'entry-title']\").text # get current name of whitepaper listed on website\n",
    "    print(\"Loading webpage \" + label)\n",
    "\n",
    "    # Scrape the embedded PDF and save it as a PDF.\n",
    "    totalcanvases = []\n",
    "    k=1\n",
    "    j=1\n",
    "    while True:\n",
    "        try:\n",
    "            #Make sure each pdf page has loaded\n",
    "            time.sleep(1)\n",
    "            #\n",
    "            #Find button for next page on pdf document\n",
    "            nextpage = driver.find_element(By.XPATH, \"//button[@class = 'pdfemb-next']\")\n",
    "            #Select the kth canvas and take a snapshot of it\n",
    "            page = driver.find_element(By.XPATH, \"//div[@class = 'pdfemb-inner-div pdfemb-page\"+str(k)+\"']//canvas\")\n",
    "            canvas_base64 = driver.execute_script(\"return arguments[0].toDataURL('image/png').substring(21);\", page)\n",
    "            canvas_png = base64.b64decode(canvas_base64)\n",
    "            with open(r\"canvas\" + str(j) + \".png\", 'wb') as f: #save to image\n",
    "                f.write(canvas_png)\n",
    "                print(\"Create \" + str(j) + \"th canvas to png.\")\n",
    "            \n",
    "            #Incremement page\n",
    "            j+=1\n",
    "            #Click on next page on pdf document\n",
    "            nextpage.click()\n",
    "            time.sleep(0.5)\n",
    "            #get how many pages we have scanned so far\n",
    "            totalcanvases.append(page)\n",
    "            k+=1\n",
    "        except Exception as e:  #We hit this exception when we get the last page. An error is thrown in\n",
    "            # the pevious chunk as the next page XPATH doesn't exist.\n",
    "            #We deal with this case in the following by not trying to find the \"next button\"\n",
    "\n",
    "            #If there are less than 3 pages scraped, log a warning\n",
    "            if k<3:\n",
    "                logging.warning(\"check \"+label + \" download.\")\n",
    "            \n",
    "            #got\n",
    "            time.sleep(0.5)\n",
    "            page = driver.find_element(By.XPATH, \"//div[@class = 'pdfemb-inner-div pdfemb-page\"+str(k)+\"']//canvas\")\n",
    "            canvas_base64 = driver.execute_script(\"return arguments[0].toDataURL('image/png').substring(21);\", page)\n",
    "            # decode\n",
    "            canvas_png = base64.b64decode(canvas_base64)\n",
    "            #save to image\n",
    "            with open(r\"canvas\" + str(j) + \".png\", 'wb') as f:\n",
    "                f.write(canvas_png)\n",
    "                print(\"Create \" + str(j) + \"th canvas to png.\")\n",
    "            j+=1\n",
    "            totalcanvases.append(page)\n",
    "            k+=1\n",
    "            break\n",
    "\n",
    "    \n",
    "    #no canvases might imply that there is no pdf embedded on the page\n",
    "    if len(totalcanvases) == 0:\n",
    "        logging.error(\"Whitepaper for \" + label + \" doesn't exist\")\n",
    "        Next_Link = driver.find_elements(By.PARTIAL_LINK_TEXT, ' Whitepaper')[-1].get_attribute(\"href\")\n",
    "        print(\"There is no link to pdf so skip \" + label)\n",
    "        continue\n",
    "\n",
    "    \n",
    "    #merge our png (derived from canvases) to pdf\n",
    "    imagelist = [\"canvas\"+ str(i) + \".png\" for i in range(1,j)]\n",
    "    pdf = FPDF()\n",
    "    k=1\n",
    "    for image in imagelist:\n",
    "        pdf.add_page()\n",
    "        pdf.image(image,0,0,210,297)\n",
    "        print(\"Added \" + str(k) +\"th image to pdf\")\n",
    "        k+=1\n",
    "\n",
    "    pdf.output(\"./whitepapers/\" + label +\".pdf\", \"F\")\n",
    "    print(\"PDF has been generated for \"+ label)\n",
    "\n",
    "    logging.info(\"PDF Creation Success for \" + str(label))\n",
    "    print(\"NEXT LINK\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
